{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b5a5cabb",
      "metadata": {
        "id": "b5a5cabb"
      },
      "source": [
        "### Clone repository and install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d37a8bdf",
      "metadata": {
        "id": "d37a8bdf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"/content/\")\n",
        "!rm -rf ASR_Conformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e1ee95aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1ee95aa",
        "outputId": "efcd525f-acab-4423-eaa7-2b24a4d1f1ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ASR_Conformer'...\n",
            "remote: Enumerating objects: 1426, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 1426 (delta 13), reused 25 (delta 11), pack-reused 1382 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1426/1426), 31.93 MiB | 10.97 MiB/s, done.\n",
            "Resolving deltas: 100% (849/849), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Torfinhell/ASR_Conformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "70279bb1",
      "metadata": {
        "id": "70279bb1"
      },
      "outputs": [],
      "source": [
        "#Going inside folder ASR_Conformer\n",
        "import os\n",
        "os.chdir(\"ASR_Conformer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f603ceed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f603ceed",
        "outputId": "9fcaefb1-c1e5-4a8d-992d-45c3b147b26f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting uv\n",
            "  Downloading uv-0.9.18-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Downloading uv-0.9.18-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uv\n",
            "Successfully installed uv-0.9.18\n",
            "Using CPython \u001b[36m3.12.0\u001b[39m\u001b[36m\u001b[39m\n",
            "Creating virtual environment at: \u001b[36m.venv\u001b[39m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m194 packages\u001b[0m \u001b[2min 4.26s\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m191 packages\u001b[0m \u001b[2min 1m 06s\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m191 packages\u001b[0m \u001b[2min 1.14s\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1maiohappyeyeballs\u001b[0m\u001b[2m==2.6.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1maiohttp\u001b[0m\u001b[2m==3.13.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1maiosignal\u001b[0m\u001b[2m==1.4.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mannotated-types\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mantlr4-python3-runtime\u001b[0m\u001b[2m==4.9.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1manyio\u001b[0m\u001b[2m==4.12.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1margparse\u001b[0m\u001b[2m==1.4.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1masttokens\u001b[0m\u001b[2m==3.0.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mattrs\u001b[0m\u001b[2m==25.4.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1maudiomentations\u001b[0m\u001b[2m==0.43.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1maudioread\u001b[0m\u001b[2m==3.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mbeautifulsoup4\u001b[0m\u001b[2m==4.14.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mblack\u001b[0m\u001b[2m==25.9.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcachetools\u001b[0m\u001b[2m==6.2.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcertifi\u001b[0m\u001b[2m==2025.11.12\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcffi\u001b[0m\u001b[2m==2.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcfgv\u001b[0m\u001b[2m==3.5.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcharset-normalizer\u001b[0m\u001b[2m==3.4.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mclick\u001b[0m\u001b[2m==8.3.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcomet-ml\u001b[0m\u001b[2m==3.53.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcomm\u001b[0m\u001b[2m==0.2.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mconfigobj\u001b[0m\u001b[2m==5.0.9\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcontourpy\u001b[0m\u001b[2m==1.3.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcryptography\u001b[0m\u001b[2m==46.0.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcycler\u001b[0m\u001b[2m==0.12.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdatasets\u001b[0m\u001b[2m==4.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdebugpy\u001b[0m\u001b[2m==1.8.19\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdecorator\u001b[0m\u001b[2m==5.2.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdill\u001b[0m\u001b[2m==0.4.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdistlib\u001b[0m\u001b[2m==0.4.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdulwich\u001b[0m\u001b[2m==0.25.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1meditdistance\u001b[0m\u001b[2m==0.8.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1meinops\u001b[0m\u001b[2m==0.8.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1meverett\u001b[0m\u001b[2m==3.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mexecuting\u001b[0m\u001b[2m==2.2.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.20.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfonttools\u001b[0m\u001b[2m==4.61.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfrozenlist\u001b[0m\u001b[2m==1.8.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.9.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgdown\u001b[0m\u001b[2m==5.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgdrive\u001b[0m\u001b[2m==0.1.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgitdb\u001b[0m\u001b[2m==4.0.12\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgitpython\u001b[0m\u001b[2m==3.1.45\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgoogle-api-core\u001b[0m\u001b[2m==2.28.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgoogle-api-python-client\u001b[0m\u001b[2m==2.43.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgoogle-auth\u001b[0m\u001b[2m==2.45.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgoogle-auth-httplib2\u001b[0m\u001b[2m==0.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgoogle-auth-oauthlib\u001b[0m\u001b[2m==0.5.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgoogleapis-common-protos\u001b[0m\u001b[2m==1.72.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mh11\u001b[0m\u001b[2m==0.16.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhf-xet\u001b[0m\u001b[2m==1.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhttpcore\u001b[0m\u001b[2m==1.0.9\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhttplib2\u001b[0m\u001b[2m==0.31.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhttpx\u001b[0m\u001b[2m==0.28.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.36.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhydra-core\u001b[0m\u001b[2m==1.3.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1midentify\u001b[0m\u001b[2m==2.6.15\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1midna\u001b[0m\u001b[2m==3.11\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mipykernel\u001b[0m\u001b[2m==7.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mipython\u001b[0m\u001b[2m==9.8.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mipython-pygments-lexers\u001b[0m\u001b[2m==1.1.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1misort\u001b[0m\u001b[2m==6.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjedi\u001b[0m\u001b[2m==0.19.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjeepney\u001b[0m\u001b[2m==0.9.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjinja2\u001b[0m\u001b[2m==3.1.6\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjoblib\u001b[0m\u001b[2m==1.5.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjsonschema\u001b[0m\u001b[2m==4.25.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjsonschema-specifications\u001b[0m\u001b[2m==2025.9.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjulius\u001b[0m\u001b[2m==0.2.7\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjupyter-client\u001b[0m\u001b[2m==8.7.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjupyter-core\u001b[0m\u001b[2m==5.9.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mkiwisolver\u001b[0m\u001b[2m==1.4.9\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlazy-loader\u001b[0m\u001b[2m==0.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlibrosa\u001b[0m\u001b[2m==0.11.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlightning-utilities\u001b[0m\u001b[2m==0.15.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mllvmlite\u001b[0m\u001b[2m==0.46.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmarkdown-it-py\u001b[0m\u001b[2m==4.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmarkupsafe\u001b[0m\u001b[2m==3.0.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmatplotlib\u001b[0m\u001b[2m==3.10.7\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmatplotlib-inline\u001b[0m\u001b[2m==0.2.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmdurl\u001b[0m\u001b[2m==0.1.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmpmath\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmsgpack\u001b[0m\u001b[2m==1.1.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmultidict\u001b[0m\u001b[2m==6.7.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmultiprocess\u001b[0m\u001b[2m==0.70.16\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmypy-extensions\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnest-asyncio\u001b[0m\u001b[2m==1.6.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnetworkx\u001b[0m\u001b[2m==3.6.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnodeenv\u001b[0m\u001b[2m==1.9.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnumba\u001b[0m\u001b[2m==0.63.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==1.26.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnumpy-minmax\u001b[0m\u001b[2m==0.3.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnumpy-rms\u001b[0m\u001b[2m==0.4.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.1.3.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.1.105\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.1.105\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.1.105\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==8.9.2.26\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.0.2.54\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.2.106\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.4.5.107\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.1.0.106\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.19.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.9.86\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.1.105\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1moauthlib\u001b[0m\u001b[2m==3.3.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1momegaconf\u001b[0m\u001b[2m==2.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpackaging\u001b[0m\u001b[2m==25.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpandas\u001b[0m\u001b[2m==2.3.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mparso\u001b[0m\u001b[2m==0.8.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpathspec\u001b[0m\u001b[2m==0.12.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpexpect\u001b[0m\u001b[2m==4.9.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==12.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mplatformdirs\u001b[0m\u001b[2m==4.5.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpooch\u001b[0m\u001b[2m==1.8.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpre-commit\u001b[0m\u001b[2m==4.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mprimepy\u001b[0m\u001b[2m==1.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mprompt-toolkit\u001b[0m\u001b[2m==3.0.52\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpropcache\u001b[0m\u001b[2m==0.4.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mproto-plus\u001b[0m\u001b[2m==1.27.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==6.33.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpsutil\u001b[0m\u001b[2m==7.1.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mptyprocess\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpure-eval\u001b[0m\u001b[2m==0.2.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyarrow\u001b[0m\u001b[2m==22.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyasn1\u001b[0m\u001b[2m==0.6.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyasn1-modules\u001b[0m\u001b[2m==0.4.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpycparser\u001b[0m\u001b[2m==2.23\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==2.12.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpydantic-core\u001b[0m\u001b[2m==2.41.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpygments\u001b[0m\u001b[2m==2.19.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyparsing\u001b[0m\u001b[2m==3.2.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpysocks\u001b[0m\u001b[2m==1.7.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpython-box\u001b[0m\u001b[2m==6.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpython-dateutil\u001b[0m\u001b[2m==2.9.0.post0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpython-stretch\u001b[0m\u001b[2m==0.3.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpytokens\u001b[0m\u001b[2m==0.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpytz\u001b[0m\u001b[2m==2025.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyyaml\u001b[0m\u001b[2m==6.0.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyzmq\u001b[0m\u001b[2m==27.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mreferencing\u001b[0m\u001b[2m==0.37.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mregex\u001b[0m\u001b[2m==2025.11.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrequests-oauthlib\u001b[0m\u001b[2m==2.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrequests-toolbelt\u001b[0m\u001b[2m==1.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrich\u001b[0m\u001b[2m==14.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrpds-py\u001b[0m\u001b[2m==0.30.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrsa\u001b[0m\u001b[2m==4.9.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msafetensors\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mscikit-learn\u001b[0m\u001b[2m==1.8.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mscipy\u001b[0m\u001b[2m==1.16.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msecretstorage\u001b[0m\u001b[2m==3.3.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msemantic-version\u001b[0m\u001b[2m==2.10.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msentry-sdk\u001b[0m\u001b[2m==2.48.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msetuptools\u001b[0m\u001b[2m==59.6.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msimplejson\u001b[0m\u001b[2m==3.20.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msix\u001b[0m\u001b[2m==1.17.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msmmap\u001b[0m\u001b[2m==5.0.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msoundfile\u001b[0m\u001b[2m==0.13.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msoupsieve\u001b[0m\u001b[2m==2.8.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msoxr\u001b[0m\u001b[2m==0.5.0.post1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mstack-data\u001b[0m\u001b[2m==0.6.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.14.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mthreadpoolctl\u001b[0m\u001b[2m==3.6.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.22.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorch-audiomentations\u001b[0m\u001b[2m==0.12.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorch-pitch-shift\u001b[0m\u001b[2m==1.2.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorchaudio\u001b[0m\u001b[2m==2.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorchmetrics\u001b[0m\u001b[2m==1.8.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.17.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtornado\u001b[0m\u001b[2m==6.5.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtqdm\u001b[0m\u001b[2m==4.67.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtraitlets\u001b[0m\u001b[2m==5.14.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.57.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==2.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtyping-extensions\u001b[0m\u001b[2m==4.15.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtyping-inspection\u001b[0m\u001b[2m==0.4.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtzdata\u001b[0m\u001b[2m==2025.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1muritemplate\u001b[0m\u001b[2m==4.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.6.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mversioneer\u001b[0m\u001b[2m==0.29\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mvirtualenv\u001b[0m\u001b[2m==20.35.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mwandb\u001b[0m\u001b[2m==0.22.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mwcwidth\u001b[0m\u001b[2m==0.2.14\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mwget\u001b[0m\u001b[2m==3.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mwheel\u001b[0m\u001b[2m==0.37.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mwrapt\u001b[0m\u001b[2m==2.0.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mwurlitzer\u001b[0m\u001b[2m==3.1.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mxxhash\u001b[0m\u001b[2m==3.6.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1myarl\u001b[0m\u001b[2m==1.22.0\u001b[0m\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=681a0f868e703c4e55a38b713bccfa36f53fa22f38c9fff28728c18296264858\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/46/3b/e29ffbe4ebe614ff224bad40fc6a5773a67a163251585a13a9\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install uv\n",
        "!uv sync\n",
        "!pip install wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "aab2f217",
      "metadata": {
        "id": "aab2f217"
      },
      "outputs": [],
      "source": [
        "#to create a run that shows augmentations\n",
        "# !uv run scripts/show_augs.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61685463",
      "metadata": {
        "id": "61685463"
      },
      "source": [
        "### Download checkpoints and dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9aad2447",
      "metadata": {
        "id": "9aad2447"
      },
      "outputs": [],
      "source": [
        "#downloading all checkpoints and example dataset with this command\n",
        "# !uv run scripts/download_gdrive.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "8b365180",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b365180",
        "outputId": "3bea31db-5b45-4bb2-ec07-9cf657ff22f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1yPsuK5BZahriXlwl87TqY6WUuxO5WSH1\n",
            "From (redirected): https://drive.google.com/uc?id=1yPsuK5BZahriXlwl87TqY6WUuxO5WSH1&confirm=t&uuid=c5339ad6-2b22-457c-b995-bd0fb7df2d78\n",
            "To: /content/ASR_Conformer/data/models/chk_train-other-500.pth\n",
            "100%|██████████| 114M/114M [00:00<00:00, 171MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ZEqEX6s7lWvCqBRclKyRW4TwJQFcxH4F\n",
            "To: /content/ASR_Conformer/data/datasets/with_gt.zip\n",
            "100%|██████████| 747k/747k [00:00<00:00, 152MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1rEynDEqEaYSL-2tI7dLTuXAPzLDCl-VY\n",
            "To: /content/ASR_Conformer/data/datasets/without_gt.zip\n",
            "100%|██████████| 745k/745k [00:00<00:00, 120MB/s]\n"
          ]
        }
      ],
      "source": [
        "#Or specify your own links like in the file and call function for downloading\n",
        "from scripts.download_gdrive import download_dataset, download_models\n",
        "GDRIVE_URLS = {\n",
        "    \"models\": {\n",
        "    #     \"https://drive.google.com/uc?id=1a1gjSXB3mMsNOHcdndhJW6ABZ8HzBEFm\": \"data/models/chk_bpeablation.pth\",  #this is not important to download, just an example\n",
        "        \"https://drive.google.com/uc?id=1yPsuK5BZahriXlwl87TqY6WUuxO5WSH1\": \"data/models/chk_train-other-500.pth\"   # train-other-500\n",
        "    },\n",
        "    \"dataset\": {# YOUR LINK HERE FOR DATASET\n",
        "        \"https://drive.google.com/uc?id=1ZEqEX6s7lWvCqBRclKyRW4TwJQFcxH4F\": \"data/datasets/with_gt\",\n",
        "        \"https://drive.google.com/uc?id=1rEynDEqEaYSL-2tI7dLTuXAPzLDCl-VY\": \"data/datasets/without_gt\"\n",
        "    }\n",
        "}\n",
        "download_models(GDRIVE_URLS)\n",
        "download_dataset(GDRIVE_URLS)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41ce06d7",
      "metadata": {
        "id": "41ce06d7"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86188fdc",
      "metadata": {
        "id": "86188fdc"
      },
      "source": [
        "##### inference on test_other librispeech dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "68c38940",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68c38940",
        "outputId": "260e0aa3-06be-409c-ebbb-a45b207a988d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/models/chk_bpeablation.pth inferenced in :\n",
            "\n",
            "Conformer(\n",
            "  (conv_subsampling): Conv2dSubsampling(\n",
            "    (subsampling): Sequential(\n",
            "      (0): Conv2d(1, 1, kernel_size=(3, 3), stride=(2, 2))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(1, 1, kernel_size=(3, 3), stride=(2, 2))\n",
            "      (3): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (linear1): Linear(in_features=19, out_features=144, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (conformer_blocks): Sequential(\n",
            "    (0): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (1): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (2): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (3): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (4): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (5): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (6): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (7): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (8): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (9): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (10): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (11): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (12): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (13): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (14): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (15): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (mlp): Linear(in_features=144, out_features=28, bias=False)\n",
            ")\n",
            "Loading model weights from: data/models/chk_bpeablation.pth ...\n",
            "inference: 100% 368/368 [1:48:33<00:00, 17.70s/it]\n",
            "    inference_CER_(Argmax):0.9838517427110244\n",
            "    inference_WER_(Argmax):0.9829849643470806\n",
            "    inference_CER_(BeamSearch):0.8730216891180629\n",
            "    inference_WER_(BeamSearch):0.9854412855056822\n",
            "data/models/chk_train-other-500.pth inferenced in :\n",
            "\n",
            "Conformer(\n",
            "  (conv_subsampling): Conv2dSubsampling(\n",
            "    (subsampling): Sequential(\n",
            "      (0): Conv2d(1, 1, kernel_size=(3, 3), stride=(2, 2))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(1, 1, kernel_size=(3, 3), stride=(2, 2))\n",
            "      (3): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (linear1): Linear(in_features=19, out_features=144, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (conformer_blocks): Sequential(\n",
            "    (0): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (1): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (2): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (3): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (4): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (5): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (6): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (7): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (8): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (9): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (10): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (11): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (12): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (13): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (14): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (15): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (mlp): Linear(in_features=144, out_features=41, bias=False)\n",
            ")\n",
            "Loading model weights from: data/models/chk_train-other-500.pth ...\n",
            "inference: 100% 368/368 [2:03:20<00:00, 20.11s/it]\n",
            "    inference_CER_(Argmax):0.18702002786740402\n",
            "    inference_WER_(Argmax):0.42566335718132203\n",
            "    inference_CER_(BeamSearch):0.18358523641502306\n",
            "    inference_WER_(BeamSearch):0.4215054862509996\n"
          ]
        }
      ],
      "source": [
        "#Inferences model on test-other  dataset and produces metrics\n",
        "# Also train-clean-100 is downloaded to create the bpe vocabulary\n",
        "#beam size can be reduced for faster inference\n",
        "\n",
        "from pathlib import Path\n",
        "model_dir = \"data/models\"\n",
        "output_dir=\"test_other_res\"\n",
        "\n",
        "for model_path in Path(model_dir).glob(\"*.pth\"):\n",
        "    print(f\"{model_path} inferenced in :\\n\")\n",
        "    if(str(model_path)==\"data/models/chk_bpeablation.pth\"):\n",
        "        !uv run inference.py \\\n",
        "            inferencer.from_pretrained={model_path} text_encoder=CTCEncoder \\\n",
        "            inferencer.save_path={output_dir} text_encoder.beam_size=100 \\\n",
        "            -cn=inference_all_metrics\n",
        "    else:\n",
        "        !uv run inference.py \\\n",
        "            inferencer.from_pretrained={model_path} text_encoder=BpeEncoder \\\n",
        "            inferencer.save_path={output_dir} text_encoder.beam_size=100 \\\n",
        "            -cn=inference_all_metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a2a7eed",
      "metadata": {
        "id": "1a2a7eed"
      },
      "source": [
        "##### inference on dataset dir with and without transcrptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ac19a534",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac19a534",
        "outputId": "2a6cbb29-f5f6-4ef2-b15b-e6179bbda448"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/models/chk_bpeablation.pth inferenced in :\n",
            "\n",
            "Conformer(\n",
            "  (conv_subsampling): Conv2dSubsampling(\n",
            "    (subsampling): Sequential(\n",
            "      (0): Conv2d(1, 1, kernel_size=(3, 3), stride=(2, 2))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(1, 1, kernel_size=(3, 3), stride=(2, 2))\n",
            "      (3): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (linear1): Linear(in_features=19, out_features=144, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (conformer_blocks): Sequential(\n",
            "    (0): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (1): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (2): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (3): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (4): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (5): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (6): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (7): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (8): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (9): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (10): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (11): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (12): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (13): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (14): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (15): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (mlp): Linear(in_features=144, out_features=28, bias=False)\n",
            ")\n",
            "Loading model weights from: data/models/chk_bpeablation.pth ...\n",
            "inference: 100% 5/5 [00:01<00:00,  3.19it/s]\n",
            "    inference_CER_(Argmax):0.9917833034076879\n",
            "    inference_WER_(Argmax):0.9836666666666666\n",
            "    inference_CER_(BeamSearch):0.9917833034076879\n",
            "    inference_WER_(BeamSearch):0.9836666666666666\n",
            "data/models/chk_train-other-500.pth inferenced in :\n",
            "\n",
            "Conformer(\n",
            "  (conv_subsampling): Conv2dSubsampling(\n",
            "    (subsampling): Sequential(\n",
            "      (0): Conv2d(1, 1, kernel_size=(3, 3), stride=(2, 2))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(1, 1, kernel_size=(3, 3), stride=(2, 2))\n",
            "      (3): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (linear1): Linear(in_features=19, out_features=144, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (conformer_blocks): Sequential(\n",
            "    (0): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (1): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (2): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (3): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (4): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (5): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (6): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (7): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (8): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (9): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (10): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (11): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (12): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (13): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (14): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (15): ConformerBlock(\n",
            "      (ffn1): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (mhsa): MultiHeadSelfAttention(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (pos_embedding): RelativePosEmb()\n",
            "        (attend): Softmax(dim=-1)\n",
            "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
            "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
            "        (to_out): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv(\n",
            "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (sequential): Sequential(\n",
            "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "          (1): GLUActivation()\n",
            "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (4): SwishActivation()\n",
            "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "          (6): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ffn2): FeedForwardNet(\n",
            "        (sequential): Sequential(\n",
            "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "          (2): SwishActivation()\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "          (5): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (mlp): Linear(in_features=144, out_features=41, bias=False)\n",
            ")\n",
            "Loading model weights from: data/models/chk_train-other-500.pth ...\n",
            "inference: 100% 5/5 [00:02<00:00,  2.05it/s]\n",
            "    inference_CER_(Argmax):0.03325782261530008\n",
            "    inference_WER_(Argmax):0.10706832298136645\n",
            "    inference_CER_(BeamSearch):0.030077832670604255\n",
            "    inference_WER_(BeamSearch):0.08884886128364387\n"
          ]
        }
      ],
      "source": [
        "#this way we inference our models on dataset_dir\n",
        "model_dir = \"data/models\"\n",
        "from pathlib import Path\n",
        "#choose type with gt or without and specify your dataset_dir, dataset_dir doesnt have to depend on gt_name\n",
        "gt_name=\"with_gt\"\n",
        "# gt_name=\"without_gt\" #metrics will return 0.0\n",
        "dataset_dir = f\"data/datasets/{gt_name}/test_data\"\n",
        "for model_path in Path(model_dir).glob(\"*.pth\"):\n",
        "    print(f\"{model_path} inferenced in :\\n\")\n",
        "    if(str(model_path)==\"data/models/chk_bpeablation.pth\"):\n",
        "        !uv run inference.py dataloader=onebatchtest \\\n",
        "            inferencer.dataset_dir={dataset_dir} \\\n",
        "            inferencer.from_pretrained={model_path} \\\n",
        "            inferencer.save_path={gt_name} \\\n",
        "            text_encoder=CTCEncoder -cn=inference\n",
        "    else:\n",
        "        !uv run inference.py dataloader=onebatchtest \\\n",
        "            inferencer.dataset_dir={dataset_dir} \\\n",
        "            inferencer.from_pretrained={model_path} \\\n",
        "            inferencer.save_path={gt_name} \\\n",
        "            text_encoder=BpeEncoder -cn=inference"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4313db2",
      "metadata": {
        "id": "c4313db2"
      },
      "source": [
        "### Calculate Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b727f071",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b727f071",
        "outputId": "7196c2ed-7d6f-4fca-8ef7-a618a93f4aec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CER Mean is 0.02988238907058401 and WER Mean is 0.09003463203463202\n"
          ]
        }
      ],
      "source": [
        "#produces cer and wer metrics for two folders with preds and gt texts\n",
        "prediction_path=f\"data/saved/{gt_name}\"\n",
        "ground_truth_path=f\"data/datasets/{gt_name}/test_data/transcriptions\"\n",
        "!uv run scripts/calculate_wer_cer.py prediction_path={prediction_path} ground_truth_path={ground_truth_path}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ba6618ef",
      "metadata": {
        "id": "ba6618ef"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}