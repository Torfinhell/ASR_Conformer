{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5a5cabb",
   "metadata": {},
   "source": [
    "### Clone repository and install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37a8bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/content/\")\n",
    "!rm -rf ASR_Conformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70279bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Going inside folder ASR_Conformer\n",
    "import os\n",
    "os.chdir(\"ASR_Conformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f603ceed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install uv\n",
    "!uv sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab2f217",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to create a run that shows augmentations\n",
    "# !uv run scripts/show_augs.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61685463",
   "metadata": {},
   "source": [
    "### Download checkpoints and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aad2447",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading all checkpoints and example dataset with this command\n",
    "# !uv run scripts/download_gdrive.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b365180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/torfinhell/Ml/DSP/ASR_Conformer/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1a1gjSXB3mMsNOHcdndhJW6ABZ8HzBEFm\n",
      "From (redirected): https://drive.google.com/uc?id=1a1gjSXB3mMsNOHcdndhJW6ABZ8HzBEFm&confirm=t&uuid=ea578f82-512c-48ea-b90a-5d0c7137993d\n",
      "To: /home/torfinhell/Ml/DSP/ASR_Conformer/data/models/chk_bpeablation.pth\n",
      "100%|██████████| 114M/114M [00:12<00:00, 8.85MB/s] \n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1yPsuK5BZahriXlwl87TqY6WUuxO5WSH1\n",
      "From (redirected): https://drive.google.com/uc?id=1yPsuK5BZahriXlwl87TqY6WUuxO5WSH1&confirm=t&uuid=61ce747f-d611-4e33-b186-faec36966468\n",
      "To: /home/torfinhell/Ml/DSP/ASR_Conformer/data/models/chk_train-other-500.pth\n",
      "100%|██████████| 114M/114M [00:11<00:00, 9.72MB/s] \n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1ZEqEX6s7lWvCqBRclKyRW4TwJQFcxH4F\n",
      "To: /home/torfinhell/Ml/DSP/ASR_Conformer/data/datasets/with_gt.zip\n",
      "100%|██████████| 747k/747k [00:00<00:00, 1.97MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1rEynDEqEaYSL-2tI7dLTuXAPzLDCl-VY\n",
      "To: /home/torfinhell/Ml/DSP/ASR_Conformer/data/datasets/without_gt.zip\n",
      "100%|██████████| 745k/745k [00:00<00:00, 1.93MB/s]\n"
     ]
    }
   ],
   "source": [
    "#Or specify your own links like in the file and call function for downloading\n",
    "from scripts.download_gdrive import download_dataset, download_models\n",
    "GDRIVE_URLS = {\n",
    "    \"models\": {\n",
    "        \"https://drive.google.com/uc?id=1a1gjSXB3mMsNOHcdndhJW6ABZ8HzBEFm\": \"data/models/chk_bpeablation.pth\",  # bpeablation\n",
    "        \"https://drive.google.com/uc?id=1yPsuK5BZahriXlwl87TqY6WUuxO5WSH1\": \"data/models/chk_train-other-500.pth\"   # train-other-500\n",
    "    },\n",
    "    \"dataset\": {# YOUR LINK HERE FOR DATASET\n",
    "        \"https://drive.google.com/uc?id=1ZEqEX6s7lWvCqBRclKyRW4TwJQFcxH4F\": \"data/datasets/with_gt\", \n",
    "        \"https://drive.google.com/uc?id=1rEynDEqEaYSL-2tI7dLTuXAPzLDCl-VY\": \"data/datasets/without_gt\"\n",
    "    }\n",
    "}\n",
    "download_models(GDRIVE_URLS)\n",
    "download_dataset(GDRIVE_URLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ce06d7",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86188fdc",
   "metadata": {},
   "source": [
    "##### inference on test_other librispeech dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68c38940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/models/chk_bpeablation.pth inferenced in :\n",
      "\n",
      "Conformer(\n",
      "  (conv_subsampling): Conv2dSubsampling(\n",
      "    (subsampling): Sequential(\n",
      "      (0): Conv2d(1, 1, kernel_size=(3, 3), stride=(2, 2))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(1, 1, kernel_size=(3, 3), stride=(2, 2))\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (linear1): Linear(in_features=19, out_features=144, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (conformer_blocks): Sequential(\n",
      "    (0): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (2): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (3): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (4): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (5): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (6): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (7): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (8): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (9): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (10): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (11): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (12): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (13): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (14): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (15): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (mlp): Linear(in_features=144, out_features=28, bias=False)\n",
      ")\n",
      "Loading model weights from: data/models/chk_bpeablation.pth ...\n",
      "inference: 100%|██████████████████████████████| 368/368 [06:45<00:00,  1.10s/it]\n",
      "    inference_CER_(Argmax):0.9838517427110247\n",
      "    inference_WER_(Argmax):0.9829849643470808\n",
      "    inference_CER_(BeamSearch):0.9838517427110247\n",
      "    inference_WER_(BeamSearch):0.9829849643470808\n",
      "data/models/chk_train-other-500.pth inferenced in :\n",
      "\n",
      "Conformer(\n",
      "  (conv_subsampling): Conv2dSubsampling(\n",
      "    (subsampling): Sequential(\n",
      "      (0): Conv2d(1, 1, kernel_size=(3, 3), stride=(2, 2))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(1, 1, kernel_size=(3, 3), stride=(2, 2))\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (linear1): Linear(in_features=19, out_features=144, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (conformer_blocks): Sequential(\n",
      "    (0): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (2): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (3): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (4): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (5): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (6): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (7): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (8): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (9): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (10): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (11): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (12): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (13): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (14): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (15): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (mlp): Linear(in_features=144, out_features=41, bias=False)\n",
      ")\n",
      "Loading model weights from: data/models/chk_train-other-500.pth ...\n",
      "inference: 100%|██████████████████████████████| 368/368 [05:12<00:00,  1.18it/s]\n",
      "    inference_CER_(Argmax):0.18705771876309746\n",
      "    inference_WER_(Argmax):0.42573654301186536\n",
      "    inference_CER_(BeamSearch):0.1845699087774895\n",
      "    inference_WER_(BeamSearch):0.42250925681752055\n"
     ]
    }
   ],
   "source": [
    "#Inferences model on test-other  dataset and produces metrics\n",
    "# Also train-clean-100 is downloaded to create the bpe vocabulary\n",
    "\n",
    "from pathlib import Path\n",
    "model_dir = \"data/models\"\n",
    "output_dir=\"test_other_res\"\n",
    "\n",
    "for model_path in Path(model_dir).glob(\"*.pth\"):\n",
    "    print(f\"{model_path} inferenced in :\\n\")\n",
    "    if(str(model_path)==\"data/models/chk_bpeablation.pth\"):\n",
    "        !uv run inference.py \\\n",
    "            inferencer.from_pretrained={model_path} text_encoder=CTCEncoder \\\n",
    "            inferencer.save_path={output_dir} text_encoder.beam_size=100 \\\n",
    "            -cn=inference_all_metrics\n",
    "    else:\n",
    "        !uv run inference.py \\\n",
    "            inferencer.from_pretrained={model_path} text_encoder=BpeEncoder \\\n",
    "            inferencer.save_path={output_dir} text_encoder.beam_size=100 \\\n",
    "            -cn=inference_all_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2a7eed",
   "metadata": {},
   "source": [
    "##### inference on dataset dir with and without transcrptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac19a534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/models/chk_bpeablation.pth inferenced in :\n",
      "\n",
      "Conformer(\n",
      "  (conv_subsampling): Conv2dSubsampling(\n",
      "    (subsampling): Sequential(\n",
      "      (0): Conv2d(1, 1, kernel_size=(3, 3), stride=(2, 2))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(1, 1, kernel_size=(3, 3), stride=(2, 2))\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (linear1): Linear(in_features=19, out_features=144, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (conformer_blocks): Sequential(\n",
      "    (0): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (2): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (3): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (4): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (5): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (6): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (7): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (8): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (9): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (10): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (11): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (12): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (13): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (14): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (15): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (mlp): Linear(in_features=144, out_features=28, bias=False)\n",
      ")\n",
      "Loading model weights from: data/models/chk_bpeablation.pth ...\n",
      "inference: 100%|██████████████████████████████████| 5/5 [00:01<00:00,  3.40it/s]\n",
      "    inference_CER_(Argmax):0.9917833034076879\n",
      "    inference_WER_(Argmax):0.9836666666666666\n",
      "    inference_CER_(BeamSearch):0.9917833034076879\n",
      "    inference_WER_(BeamSearch):0.9836666666666666\n",
      "data/models/chk_train-other-500.pth inferenced in :\n",
      "\n",
      "Conformer(\n",
      "  (conv_subsampling): Conv2dSubsampling(\n",
      "    (subsampling): Sequential(\n",
      "      (0): Conv2d(1, 1, kernel_size=(3, 3), stride=(2, 2))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(1, 1, kernel_size=(3, 3), stride=(2, 2))\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (linear1): Linear(in_features=19, out_features=144, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (conformer_blocks): Sequential(\n",
      "    (0): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (2): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (3): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (4): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (5): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (6): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (7): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (8): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (9): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (10): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (11): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (12): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (13): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (14): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (15): ConformerBlock(\n",
      "      (ffn1): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (pos_embedding): RelativePosEmb()\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=144, out_features=768, bias=True)\n",
      "        (proj_emb): Linear(in_features=144, out_features=256, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=144, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv(\n",
      "        (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequential): Sequential(\n",
      "          (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLUActivation()\n",
      "          (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
      "          (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): SwishActivation()\n",
      "          (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
      "          (6): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn2): FeedForwardNet(\n",
      "        (sequential): Sequential(\n",
      "          (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (2): SwishActivation()\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "          (4): Linear(in_features=576, out_features=144, bias=True)\n",
      "          (5): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (mlp): Linear(in_features=144, out_features=41, bias=False)\n",
      ")\n",
      "Loading model weights from: data/models/chk_train-other-500.pth ...\n",
      "inference: 100%|██████████████████████████████████| 5/5 [00:00<00:00,  5.60it/s]\n",
      "    inference_CER_(Argmax):0.03325782261530008\n",
      "    inference_WER_(Argmax):0.10706832298136645\n",
      "    inference_CER_(BeamSearch):0.030077832670604255\n",
      "    inference_WER_(BeamSearch):0.08884886128364387\n"
     ]
    }
   ],
   "source": [
    "#this way we inference our models on dataset_dir\n",
    "model_dir = \"data/models\"\n",
    "from pathlib import Path\n",
    "#choose type with gt or without and specify your dataset_dir, dataset_dir doesnt have to depend on gt_name\n",
    "gt_name=\"with_gt\"\n",
    "# gt_name=\"without_gt\" #metrics will return 0.0\n",
    "dataset_dir = f\"data/datasets/{gt_name}/test_data\"\n",
    "for model_path in Path(model_dir).glob(\"*.pth\"):\n",
    "    print(f\"{model_path} inferenced in :\\n\")\n",
    "    if(str(model_path)==\"data/models/chk_bpeablation.pth\"):\n",
    "        !uv run inference.py dataloader=onebatchtest \\\n",
    "            inferencer.dataset_dir={dataset_dir} \\\n",
    "            inferencer.from_pretrained={model_path} \\\n",
    "            inferencer.save_path={gt_name} \\\n",
    "            text_encoder=CTCEncoder -cn=inference\n",
    "    else:\n",
    "        !uv run inference.py dataloader=onebatchtest \\\n",
    "            inferencer.dataset_dir={dataset_dir} \\\n",
    "            inferencer.from_pretrained={model_path} \\\n",
    "            inferencer.save_path={gt_name} \\\n",
    "            text_encoder=BpeEncoder -cn=inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4313db2",
   "metadata": {},
   "source": [
    "### Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b727f071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CER Mean is 0.02988238907058401 and WER Mean is 0.09003463203463202\n"
     ]
    }
   ],
   "source": [
    "#produces cer and wer metrics for two folders with preds and gt texts\n",
    "prediction_path=f\"data/saved/{gt_name}\"\n",
    "ground_truth_path=f\"data/datasets/{gt_name}/test_data/transcriptions\"\n",
    "!uv run scripts/calculate_wer_cer.py prediction_path={prediction_path} ground_truth_path={ground_truth_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6618ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr-conformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
